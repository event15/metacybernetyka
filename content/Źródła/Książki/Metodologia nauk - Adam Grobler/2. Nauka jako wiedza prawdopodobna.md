---
tags:
  - input/książka
author: "[[Adam Grobler]]"
dotyczy:
  - "[[metodologia]]"
---
- Metoda indukcji eliminacyjnej miała na celu przekształcenie zawodnego wnioskowania indukcyjnego w coś równie niezawodnego jak wnioskowanie dedukcyjne. Okazuje się jednak, że nawet najbogatsze, lecz z natury rzeczy skończone świadectwo empiryczne nie może wystarczyć do wyczerpującego uzasadnienia żadnej hipotezy uniwersalnej. (s. 33) ^617aa7
- Świadectwo empiryczne zatem przynajmniej częściowo uzasadnia wybór hipotez. Takie rozumowanie prowadzi do stanowiska nazywanego przez Lakatosa[^1] **słabym justyfikacjonizmem**. (s. 33)
- Żadna hipoteza nigdy nie jest wyczerpująco uzasadniona, ale może być lepiej lub gorzej potwierdzona przez świadectwo. Potwierdzenie jest więc stopniowalne (s. 34) ^2b9592
- Jak mierzyć stopień potwierdzenia? Są dwa poważne argumenty, aby stopień potwierdzenia hipotezy traktować jako prawdopodobieństwo jej prawdziwości, dokładniej, prawdopodobieństwo warunkowe ze względu na świadectwo. (s. 34) ^e50e9a
	- Po pierwsze, za pomocą pojęcia prawdopodobieństwa warunkowego można zdefiniować relację między zdaniami[^2] - z których jedno wyraża hipotezę, a drugie świadectwo - będącą uogólnieniem zwykłej implikacji. Mianowicie: równość $P(H|E) = 1$ znaczy tyle, że jeżeli zachodzi $E$ ($E$ jest zdaniem prawdziwym), to $P(H) = 1$, czyli $H$ jest pewne[^3], a zatem prawdziwe. Innymi słowy, $P(H|E) = 1$ jest praktycznie równoważne implikacji $E \implies H$. Z kolei równość $P(H|E) = 0$ znaczy tyle, że jeżeli zachodzi $E$ ($E$ jest zdaniem prawdziwym), to $P(H) = 0$, czyli jest niemożliwe[^4], to znaczy fałszywe. Innymi słowy, $P(H|E) = 0$ jest w zasadzie równoważne implikacji $E \implies \lnot H$. Przypadki pośrednie, to znaczy $P(H|E) = r, 0 < r < 1$, reprezentują więc sytuację, w której $E$ "częściowo implikuje" $H$, ale ją niejako sugeruje: im większe $r$, tym bardziej prawdopodobne, że $H$ jest prawdziwe. Wydaje się, że świadectwo empiryczne wchodzi w tego rodzaju relacje z hipotezami: nie implikuje ich, lecz uprawdopodabnia.
	- Po drugie, wydaje się, że za pomocą pojęcia prawdopodobieństwa można należycie określić **stopień racjonalnego przekonania** o prawdziwości hipotezy. Z braku wyczerpujących dowodów uczony nigdy nie może być pewny swoich hipotez. Stąd jednak nie wynika, że musi, jak starożytny sceptyk, przestrzegać zasady równowagi sądzenia. Przeciwnie, świadectwa słusznie skłaniają uczonych do dyskryminacji hipotez: uczeni są w mniejszym lub większym stopniu przekonani o ich prawdziwości. ^94bd8b
- Pytanie brzmi: co to znaczy, że ktoś jest przekonany w stopniu $r$ o prawdziwości $H$? Stopień przekonania najlepiej jest mierzyć skłonnością do działania na podstawie danej hipotezy. (s. 35)
- Warunkiem racjonalności gracza zawierającego zakłady jest to, by przystępując do systemu zakładów, nie przegrał. (s. 35)
- Okazuje się, że kompletny system zakładów (to jest taki, w którym gracz obstawia wszystkie możliwe wyniki) nie jest systemem holenderskim wtedy i tylko wtedy, gdy układ stopni przekonania gracza, mierzony jest ilorazem zawieranych przez niego zakładów, spełnia aksjomaty rachunku prawdopodobieństwa[^5]. (s. 35)
- Z powyższych rozważań wynika, że wyjściowy układ stopni przekonania powinien spełniać aksjomaty rachunku prawdopodobieństwa, a następnie być modyfikowany pod wpływem świadectwa, również zgodnie z aksjomatami rachunku prawdopodobieństwa. Ten drugi warunek jest spełniony, jeżeli stopień przekonania o prawdziwości hipotezy ze względu na dane świadectwo ujmuje się jako prawdopodobieństwo warunkowe. (s. 36)
- Kłopot polega na tym, że samo pojęcie prawdopodobieństwa nie jest całkiem jasne.
- Z czysto matematycznego punktu widzenia prawdopodobieństwo jest unormowaną miarą addytywną określoną na pewnym zbiorze, to jest spełniającą pewne dodatkowe warunki funkcją o wartościach z przedziału $[0,1]$, określoną na spełniającym pewne warunki podzbiorze zbioru podzbiorów pewnego zbioru.(s. 36)
- Natomiast z filozoficznego punktu widzenia potrzebna jest jakaś interpretacja umożliwiająca wyjaśnienie, dlaczego to matematyczne pojęcie ma zastosowanie do rozwiązywania określonego typu problemów. W wypadku zastosowania do problemu indukcji rozpatrywano trzy interpretacje: częstościową, logiczną i subiektywną. (s. 36) ^af3330
- **Interpretacja częstościowa** nawiązuje do tak zwanej klasycznej definicji prawdopodobieństwa[^6], wyrażającej prostą intuicję, wedle której prawdopodobieństwo interesującego nas zdarzenia jest miarą jego średniej częstości w nieograniczenie długiej serii zdarzeń określonego typu. (s. 36) ^32155e
- Nieco zagadkowe pojęcie **częstości**, nie dość, że średniej, to jeszcze w nieograniczenie długiej serii zdarzeń, ma całkiem ścisłą, matematyczną definicję. Mianowicie: nieograniczenie długą serię zdarzeń można przedstawić w formie nieskończonego ciągu skończonych ciągów $n$ początkowych zdarzeń tej serii. W każdej takiej skończonej podserii częstość - nazwijmy ją względną - interesującego nas zdarzenia można określić po prostu jako stosunek liczby wystąpień tego zdarzenia w danej podserii do długości tej podserii na przykład stosunek liczby reszek do $n$ - liczby rzutów w podserii $n$ początkowych rzutów nieograniczenie długiej serii rzutów monetą. (s. 36, s. 37)
- W szczęśliwym wypadku, w którym ciąg częstości względnych jest zbieżny, jego granicę można nazwać średnią częstością interesującego nas zdarzenia. W pozostałych wypadkach średnia częstość nie jest określona. (s. 37)
- Interpretację częstościową prawdopodobieństwa próbował zastosować do budowy logiki indukcji Hans Reichenbach[^7]. Ten pomysł ma jednak wiele wad. Po pierwsze wymaga traktowania hipotez jako zdań eliptycznych o prawdopodobieństwie. Znaczy to, że hipotezę postaci $(\forall x)[W(x) \to Z(x)]$  należy rozumieć nie dosłownie (każdy przedmiot, który znajdzie się w warunkach $W$, zachowa się w sposób $Z$), lecz przenośnie: przedmiot, który znajdzie się w warunkach $W$, prawdopodobnie zachowa się w sposób $Z$. Po drugie, na co wskazywał von Mises[^8], wartości prawdopodobieństwa w interpretacji częstościowej można szacować jedynie na podstawie doświadczenia. Same szacunki zatem mają charakter hipotez indukcyjnych. Wobec tego stosowanie pojęcia prawdopodobieństwa w interpretacji częstościowej do uzasadniania procedur indukcyjnych zakrawa na błędne koło. Po trzecie, z czysto matematycznego punktu widzenia średnia częstość zależy od kolejności wyrazów ciągu względnych częstości. To znaczy, że przygodna zmiana kolejności rejestrowania świadectw, bez zmiany samych świadectw, może mieć istotny wpływ na ocenę prawdopodobieństwa hipotezy. (s. 37) ^fc4af1
- Na idei prawdopodobieństwa logicznego opiera się sformułowany przez Carnapa program logiki indukcji. Prawdopodobieństwo warunkowe zdania $H$ ze względu na zdanie $E$ jest w tym sensie prawdopodobieństwem logicznym, że zależy od tak zwanego prawdopodobieństwa *a priori* (bez dowodu) zdań $H$ i $E$ oraz czysto logicznego związku między nimi, związku będącego uogólnieniem klasycznej implikacji. Własności tego związku można zatem opisać za pomocą odpowiedniego rachunku logicznego. (s. 37, s.38)
- Konstrukcja takiego rachunku, sformułowana w *Logical Foundations of Probability*[^9], przebiega następująco.
	- Punktem wyjścia jest monadyczny język rachunku predykatów pierwszego rzędu, to jest język, którego alfabet obejmuje:
		- zmienne i stałe indywiduowe: $x, y, z, \dots, a_1, \dots, a_N$,
		- predykaty jednoargumentowe: $P_1, \dots, P_r$,
		- spójniki logiczne: $\lnot, \land, \lor, \to (\implies), \leftrightarrow (\iff)$,
		- kwantyfikatory: $\exists, \forall$ 
	- Dodatkowo zakłada się, że w języku występują nazwy własne wszystkich elementów jego uniwersum i każdy element uniwersum ma tylko jedną nazwę. Innymi słowy, skoro w języku znajduje się $N$ stałych indywiduowych, uniwersum języka składa się z $N$ indywiduów. Ponadto o predykatach $P_1, \dots, P_r$ zakłada się, że są proste, to znaczy żaden z nich nie daje się zdefiniować za pomocą innych predykatów. Natomiast za pomocą predykatów prostych można zdefiniować szczególną klasę predykatów, tak zwanych Q-predykatów: 
	  $$Q_i(x) \iff \pm P_1 \land \pm P_2 (x) \land \dots \land \pm P_r(x), i = 1, \dots, k, k = 2^r$$
	  gdzie symbol $\pm P_j(x)$ oznacza bądź $P_j(x)$, bądź $\lnot P_j(x), j = 1, \dots, r$.
	  Szczególna rola Q-predykatów polega na tym, że gdy w miejsce zmiennej $x$ podstawić nazwę określonego indywiduum, powiedzmy: $a$, Q-predykat orzeka o nim, które z własności oznaczonych za pomocą predykatów elementarnych mu przysługują, a które nie. Innymi słowy, podaje jego wyczerpujący opis - wyczerpujący ze względu na siłę wyrazu rozpatrywanego języka. Naturalnie, każdy Q-predykat podaje inny opis $a$, a więc tylko jeden z nich może być prawdziwy. W ten sposób Q-predykaty wyznaczają wyczerpujący i rozłączny podział uniwersum języka na Q-zbiory: zbiory "jednakowych" indywiduów, to jest indywiduów spełniających ten sam Q-predykat. Każdemu możliwemu podziałowi uniwersum na Q-zbiory można przypisać ciąg Q-liczb, $N_1, \dots, N_k, (N_1 + \dots + N_k = N)$, określających liczebność poszczególnych Q-zbiorów, czyli, innymi słowy, rozkład statystyczny indywiduów na poszczególne Q-zbiory. Za pomocą Q-predykatów można zbudować szczególnego rodzaju zdania, zwane opisami indywiduowymi. Mają one postać następującą: 
	  $$ S_{i} \iff Q_{i_{1}}(a_{i}) \land \dots \land Q_{i_{N}}(a_{N}), i = 1, \dots, k^{N}, i_{j} = 1, \dots, k.$$
	  Zdanie tego typu jest wyczerpującym opisem uniwersum, ponieważ jest koniunkcją wyczerpujących opisów wszystkich indywiduów. Każdy opis indywiduowy definiuje pewien "możliwy świat", to jest świat, o którym ten opis jest prawdziwy. W szczególności każdemu opisowi indywiduowemu jednoznacznie odpowiada pewien ciąg Q-liczb (ale nie na odwrót: różnym opisom indywiduowym może odpowiadać ten sam ciąg Q-liczb). Każde inne zdanie rozpatrywanego języka daje się przedstawić w postaci alternatywy opisów indywiduowych. Każde dwa opisy indywiduowe wykluczają się wzajemnie, co ma doniosłe znaczenie dla dalszej konstrukcji rachunku. Wystarczy mianowicie określić prawdopodobieństwo *a priori* dla wszystkich opisów, aby uznać - na mocy aksjomatu $P(A \lor B) = P(A) + P(B)$, gdy $A$ i $B$ się wykluczają - rozkład prawdopodobieństwa na zbiorze wszystkich zdań rozpatrywanego języka. (s. 38, s. 39)
- Jak jednak to prawdopodobieństwo określić? Z aksjomatów rachunku prawdopodobieństwa wynika, że musi być spełniony warunek: $P(S_1) + \dots + P(S_{k^N}) = 1$. Idąc po linii najmniejszego oporu, można by przyjąć, zgodnie z tak zwaną zasadą racji niedostatecznej, że wszystkie opisy indywiduowe są równoprawdopodobne, czyli że $P(S_1) = \frac{1}{k^N}$ dla każdego $i$. Byłoby to jednak równoznaczne - ponieważ opisy indywiduowe są symetryczne ze względu na wszystkie Q-predykaty i nazwy wszystkich indywiduów - z założeniem, że każdy rozkład własności na poszczególne indywidua jest jednakowo prawdopodobny. Wówczas każde dwa zdania logiczne niezależne byłyby zarazem probabilistycznie niezależne, czyli prawdopodobieństwo warunkowe dowolnego zdania ze względu na jakiekolwiek świadectwo empiryczne logicznie od niego niezależne byłoby równe prawdopodobieństwu *a priori* tego zdania. Taki rachunek nie pozwalałby więc na wprowadzenie żadnych interesujących wniosków indukcyjnych. (s. 39)
- Żeby więc można było zbudować rachunek logiczny, który dawałby podstawy do wyciągania wniosków indukcyjnych, trzeba przypisać opisom indywiduowym niejednakowe prawdopodobieństwa *a priori*. Ale jak? Carnap uznał, że w tym celu wpierw należy określić prawdopodobieństwo zdań innego typu, zdań zwanych opisami strukturalnymi albo statystycznymi. Są to zdania o postaci alternatywy wszystkich opisów indywiduowych, którym odpowiada ten sam ciąg Q-liczb. Opisy statystyczne mówią zatem o tym, ile indywiduów należy do każdego z Q-zbiorów, to jest spełnia określony Q-predykat, czyli ma określony asortyment elementarnych własności (własności oznaczonych za pomocą predykatów elementarnych). Mówią zatem o świecie wszystko to, co da się o nim właśnie w stosunku do opisów statystycznych Carnap zastosował zasadę racji niedostatecznej, to jest przypisał im jednakowe prawdopodobieństwa. (s. 40)
- Prawdopodobieństwo opisu statystycznego, zgodnie z aksjomatami rachunku prawdopodobieństwa, musi być sumą prawdopodobieństw opisów indywiduowych, których jest alternatywą. Teraz można przyjąć zasadę racji niedostatecznej do opisów indywiduowych będących składnikami tego samego opisu statystycznego i przypisać im jednakowe prawdopodobieństwa. (s. 40)
- Widać stąd wyraźnie, że prawdopodobieństwo *a priori* opisu indywiduowego będzie odwrotnie proporcjonalne do liczby opisów indywiduowych o tym samym ciągu Q-liczb. Będzie więc ono tym większe, im bardziej ciąg Q-liczb będzie "niezrównoważony", to znaczy im bardziej jego wyrazy będą różniły się od przeciętnej[^10]. (s. 40)
- Największe zaś będzie dla ciągów, których jeden wyraz wynosi $N$, a pozostałe są równe zero. (s. 40)
- W takim razie opisy indywiduowe są tym bardziej *a priori* prawdopodobne, im większej liczbie indywiduów przypisują więcej podobieństw (wspólnych własności elementarnych). Najbardziej prawdopodobne *a priori* są te opisy indywiduowe, które mówią, że wszystkie indywidua są jednakowe (mają te same własności elementarne). Taki rozkład prawdopodobieństwa przypisuje więc *a priori* wyższe prawdopodobieństwo temu, że w świecie występują (statystyczne) związki współwystępowania (lub wykluczania się) między poszczególnymi własnościami elementarnymi, niż temu ,że takich związków absolutnie brak; i to tym wyższe prawdopodobieństwo, im więcej jak takich związków współwystępowania (i im one są statystycznie istotniejsze). Takie założenie można uznać za statystyczną wersję zasady jednostajności przyrody. (s. 40)
- Oznaczmy teraz przez $L_{N,k}$ język, który zawiera $N$ nazw indywiduowych i $k$ Q-predykatów. Określmy na zbiorze zdań tego języka rozkład prawdopodobieństwa zgodnie ze sformułowaną wyżej zasadą racji niedostatecznej dla opisów statystycznych oraz zasadą racji niedostatecznej dla opisów indywiduowych, którym odpowiadają te same ciągi Q-liczb. Wówczas można zdefiniować funkcję potwierdzenia hipotezy $H$ sformułowanej w tym języku przez świadectwo empiryczne $E$ (zdanie opisujące to świadectwo) znanym wzorem na prawdopodobieństwo warunkowe:[^11] $$ C^*(H|E) = \frac{P(H \land E)}{P(E)}$$ (s. 41)
- W nauce interesuje nas przede wszystkim potwierdzanie hipotez uniwersalnych, to jest hipotez o ogólnej postaci $(\forall{x})[W(x) \implies Z(x)]$. W wypadku języka o skończonej liczbie nazw indywiduowych, w którym każdy element uniwersum ma nazwę indywiduową - a takie języki na razie rozpatrujemy - hipotezy uniwersalne redukują się do znacznie mniej interesujących zdań szczegółowych o postaci skończonej koniunkcji: $[W(a_1) \implies Z(a_1)] \land \dots \land [W(a_N) \implies Z(a_N)]$. Żeby można było rozważać kwestię potwierdzania hipotez uniwersalnych, trzeba dokonać uogólnienia funkcji potwierdzania na języki o nieskończonych uniwersach. W tym celu Carnap zdefiniował prawdopodobieństwo *a priori* zdania języka $L_{\infty, k}$ - to jest monadycznego języka rachunku predykatów pierwszego rzędu o nieskończonym uniwersum i $k$ Q-predykatach - jako granicę ciągu prawdopodobieństw tego zdania w językach $L_{N,k}$, przy rosnącym $N$. (s. 41)
- Okazuje się, że przy takiej definicji prawdopodobieństwo *a priori* hipotez ściśle uniwersalnych (to jest nieredukujących się do skończonej koniunkcji zdań szczegółowych) jest równe zero. Wynika stąd, że prawdopodobieństwo warunkowe dowolnej hipotezy ściśle uniwersalnej ze względu na dowolne świadectwo empiryczne (które wyraża się zawsze za pomocą zdania szczegółowego) jest również równe zero. Wniosek to dość osobliwy, mówi on bowiem ni mniej, ni więcej jak to, że potwierdzenie dowolnej hipotezy uniwersalnej przez dowolne skończone świadectwo empiryczne (a każde świadectwo empiryczne jest skończone) jest zerowe. (s. 42)
- W wypadku hipotez uniwersalnych załamuje się również projektowane uogólnienie logiki dedukcyjnej, o którym była mowa w poprzednim rozdziale. Mianowicie: gdy $G \implies H$, to powinna zachodzić równość $P(H|G) = 1$. Tymczasem jeżeli $G$ i $H$ są zdaniami uniwersalnymi, to $P(H|G)$ jest nieokreślone. Można więc powiedzieć, że zdania uniwersalne wymykają się spod ogólnych założeń programu logiki indukcji. (s. 42)
- W wypadku gier w rodzaju rzutów monetą najczęściej przyjmujemy, że wyniki kolejnych rzutów są od siebie statystycznie niezależne i prawdopodobieństwo wyrzucenia orła za każdym razem wynosi $\frac{1}{2}$. To znaczy tak przyjmujemy, gdy zakładamy, że gra jest uczciwa (...).
- Takie podejście do zagadnienia naśladuje sądową zasadę domniemania niewinności: trzeba mieć jakieś specjalne powody, by podejrzewać, że moneta jest nieprawidłowa lub mają miejsce jakieś tajemne manipulacje. Natomiast stosując logikę indukcji, dajemy wyraz przekonaniu, że samo odchylenie liczby orłów od przeciętnej skłania do podejrzeń, w stopniu zależnym, aczkolwiek nie wprost, od wielkości tego odchylenia. 
- Z powyższych przykładów wyraźnie widać, że stosowanie logiki Carnapa jest uzasadnione pod warunkiem, że założenie o dodatniej zależności statystycznej między badanymi zdarzeniami jest wiarygodne. Nawet wtedy jednak powstaje pytanie, jak wysoka jest ta zależność statystyczna, to jest, jak bardzo świadectwo empiryczne wpływa na ocenę prawdopodobieństwa spełnienia się przewidywania. Ten problem Carnap próbował rozwiązać, wprowadzając w *The Continuum of Inductive Methods* "kontinuum metod indukcyjnych", które uzależnia funkcję potwierdzania od wartości dodatkowego parametru $\lambda(k)$, gdzie $\lambda$ jest funkcją $k$, $k$ jest liczbą Q-predykatów. Wzór przedstawia się następująco:$$C(Q_{i}(a_{n+1})|E) = \frac{n_i+\frac{\lambda(k)}{k}}{n+\lambda(k)}$$  (s 44)
- WIelkość $\lambda$ wyraża skłonność do wyciągania wniosków indukcyjnych. Gdy $\lambda = 0$, funkcja potwierdzania jest równa po prostu $\frac{n_i}{n}$, czyli stosunkowi liczby wystąpień badanej własności do liczby zaobserwowanych przypadków. Gdy $\lambda \implies \infty$, funkcja potwierdzania zmierza do $\frac{1}{k}$, to jest do prawdopodobieństwa *a priori* niezależnego od świadectwa. W ten sposób Carnap sugeruje, że dobór odpowiedniej wartości parametru $\lambda$ zależy od rodzaju zagadnienia, do którego stosuje się logikę indukcji. (s. 45)
- Wartość $\lambda$ zależy od założenia na temat zależności statystycznej rozpatrywanych zdarzeń. Założenie to tkwi w definicji prawdopodobieństwa *a  priori*, wyjściowym rozkładzie prawdopodobieństwa na zbiorze zdań rozpatrywanego języka. Jasne jest, że w wyborze wyjściowego rozkładu prawdopodobieństwa nie możemy kierować się logiką indukcji, bo to od niego zależy cały rachunek logiczny. Chyba żeby rozważyć całą klasę możliwych rozkładów prawdopodobieństwa *a priori* jako zbiór hipotez sformułowanych w pewnym metajęzyku, dla którego można zbudować rachunek logiki indukcji niejako drugiego rzędu i zastosować go do wyboru rozkładu prawdopodobieństwa *a priori* na zbiorze zdań języka przedmiotowego. Wówczas jednak powstaje problem wyboru wyjściowego rozkładu prawdopodobieństwa na zbiorze zdań metajęzyka, który albo pozostaje bez rozwiązania, albo do jego rozwiązania zostanie zastosowany rachunek logiki indukcji dla odpowiedniego metametajęzyka - i tak dalej w nieskończoność. Krótko mówiąc, powstaje dylemat: albo wybór wyjściowego rozkładu prawdopodobieństwa jest całkowicie arbitralny, albo powstaje regres w nieskończoność. Próbę ominięcia tego problemu podjął bayesianizm. (str. 45)
- Bayesianizm opiera się na subiektywnej koncepcji prawdopodobieństwa, to jest prawdopodobieństwa nie jako miary stopnia obiektywnego potwierdzenia hipotezy, lecz jako miary stopnia subiektywnego przekonania uczonego o prawdziwości hipotezy. Obiektywność w metodzie opartej na tej koncepcji zapewnia mechanizm modyfikowania prawdopodobieństw pod wpływem świadectwa empirycznego. Polega on na systematycznym stosowaniu **twierdzenia Bayesa**: $$ P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)} = \frac{P(E|H) \cdot P(H)}{P(E|H) \cdot P(H) + P(E|\lnot H) \cdot P(\lnot H)}$$
- To znaczy: stosujący metodę określa wyjściowy rozkład prawdopodobieństwa na zbiorze zdań danego języka w sposób zupełnie dowolny, byle zgodny z aksjomatami rachunku prawdopodobieństwa. Następnie po uzyskaniu świadectwa empirycznego $E$, określa rozkład prawdopodobieństwa na nowo, przypisując każdej hipotezie prawdopodobieństwo równe jej prawdopodobieństwu warunkowemu ze względu na $E$, obliczone według wzoru Bayesa. (s. 46)
- Regułę postępowania, wedle której racjonalny badacz powinien po otrzymaniu świadectwa empirycznego każdorazowo modyfikować swój rozkład prawdopodobieństwa zgodnie z twierdzeniem Bayesa, nazywa się **regułą warunkowania** (*conditionalization rule*). (s. 46)
- Mechanizm modyfikowania prawdopodobieństw wedle reguły warunkowania jest jasny i jednoznaczny. Wątpliwości natomiast może budzić to, że wyniki stosowania tej reguły przez różnych badaczy mogą być różne, ponieważ określenie wyjściowego rozkładu prawdopodobieństwa jest całkowicie dowolne. (s. 47)
- Jest jednak faktem matematycznym, że różnice w ocenie prawdopodobieństwa po wielokrotnym zastosowaniu twierdzenia Bayesa do różnych wyjściowych rozkładów prawdopodobieństwa maleją do zera, gdy liczba zastosowań tego twierdzenia rośnie do nieskończoności. (s. 47)
- Arbitralność wyboru punktu wyjścia, tak kłopotliwa dla programu Carnapa, z bayesiańskiego punktu widzenia nie przeszkadza uprawomocnieniu indukcji za pomocą metody szacowania prawdopodobieństwa hipotez. (s. 48)
- Przypuśćmy, że wybieramy się samochodem z Krakowa do Warszawy i z jakichś powodów wolimy po drodze nie tankować paliwa. Przyjmijmy, że dla samochodu startującego z pełnym bakiem prawdopodobieństwo dotarcia do celu bez tankowania wynosi $70\%$, a dojechania przynajmniej do Grójca - $90\%$. Pytanie: czy minąwszy Grójec, wolno nam wnosić, na mocy reguły warunkowania, że prawdopodobieństwo sukcesu wzrosło do $\frac{7}{9}$? Czego takiego dowiedzieliśmy się, dotarłszy do Grójca, co wzmacnia nasze nadzieje? Bas van Frassen (w korespondencji z Plackiem) przedstawił następujący argument na korzyść pozytywnej odpowiedzi. Na mocy wyjściowego założenia do Warszawy dojedzie siedem samochodów na dziesięć, a z tych dziesięciu jeden utknie już przed Grójcem. Minąwszy Grójec, wiemy, że nasz samochód jest jednym z dziewięciu, z których siedem dotrze do Warszawy. Rozumowanie to byłoby całkiem trafne, gdyby proces był zupełnie losowy. (s. 48)
- Z powyższej dyskusji wynika, że aby stosować twierdzenie Bayesa z pożytkiem, trzeba posługiwać się wiedzą na temat poziomu istotności świadectw dla rozważanej hipotezy. W przykładzie samochodowym posłużyliśmy się wiedzą potoczną na temat ogólnych mechanizmów spalania paliwa. W innych wypadkach będzie podobnie: ocena użyteczności świadectw dla szacowania prawdopodobieństwa danej hipotezy, a tym samym ocena wartości poznawczej oszacowania tego prawdopodobieństwa, będzie zależała od jakiejś wcześniejszej wiedzy, wiedzy empirycznej na temat stałych związków między zjawiskami. Tego rodzaju wiedzę nabywamy metodą indukcji. Gdyby bayesianizm był trafną rekonstrukcją metody indukcyjnej, owa wcześniejsza wiedza byłaby również wiedzą prawdopodobną. (s. 51)
- Ocena trafności doboru świadectw do oszacowania prawdopodobieństwa wcześniejszej wiedzy zależałaby od jakiejś jeszcze wcześniejszej wiedzy, która również byłaby tylko prawdopodobna, i tak dalej. Znowu otrzymujemy regres w nieskończoność. (s. 51)
- Wydaje się, że jest to nieodłączna cecha każdej propozycji rozwiązania problemu indukcji w duchu probabilizmu. Prawdopodobieństwa bowiem nie spadają z nieba. Zależności probabilistyczne między zdaniami wynikają z pewnych założeń. Te założenia same podlegają ocenie ze względu na stopień wiarygodności. Jeżeli stopień wiarygodności hipotezy zależy od oceny jej prawdopodobieństwa, powstaje regres w nieskończoność. (s. 51)
- Zarówno program Carnapa, jak i bayesianizm dążą do zrekonstruowania czegoś, co zgodnie, aczkolwiek niekoniecznie trafnie, nazywają logiką nauki, czyli teorią idealnych wzorców postępowania naukowego w zakresie oceny wartości poznawczej hipotez. (s. 52)
- Poważniejszym problemem jest kwestia oceny hipotez uniwersalnych. W logikach Carnapa, o czym była mowa wyżej, stopień potwierdzenia dowolnej hipotezy uniwersalnej przez jakiekolwiek świadectwo (zdanie szczegółowe) jest równy zero. (s. 52)
- Krótko mówiąc, żadne świadectwo empiryczne nie jest pewne i - jak powiedział Clawrence I. Lewis - jeżeli nie ma nic pewnego, nie ma też nic prawdopodobnego. (s. 60)
- Karl R. Popper uważał nie tylko, że próby rozwiązania problemu indukcji za pomocą teorii potwierdzania są nieudane, lecz również, że nie mogą się udać z pewnego zasadniczego powodu. Mianowicie dążenie do potwierdzania hipotez w ogóle nie jest postawą godną uczonego. Potwierdzanie nie jest metodą nauki, lecz *pseudonauki*.

[^1]: Zob. I. Lakatos, *Falsyfikacja a metodologia naukowych programów badawczych* w: tenże, *Pisma z filozofii nauk empirycznych*, tłum. W. Sady, Warszawa 1995 (pierwodruk oryginału 1970)
[^2]: Czytelnik może być przyzwyczajony do tego, że prawdopodobieństwo określa się nie na zdaniach, ale na zdarzeniach pojmowanych jako zbiory. W istocie nie ma wielkiej różnicy: wystarczy zbiór zastąpić zdaniem będącym jego opisem, a operacje mnogościowe zastąpić logicznymi. W szczególności prawdopodobieństwo warunkowe hipotezy $H$ ze względu na świadectwo $E$ definiuje się wzorem: $P(H|E) = \frac{P(H \land E)}{P(E)}$.
[^3]: przy założeniu, ze przestrzeń prawdopodobieństwa jest skończona. W przeciwnym razie $P(H) = 1$ jest równoważne temu, że zbiór przypadków sprzyjających $nie-H$ jest miary zero (lecz może być niepusty).
[^4]: Dokładniej: zbiór przypadków sprzyjających $H$ jest miary zero (choć może być niepusty).
[^5]: Zanim Kołmogorow sformułował klasyczny układ aksjomatów, Thomas Bayes, jeden z pionierów rachunku prawdopodobieństwa, używał argumentu z holenderskiego systemu zakładów dla uzasadnienia swoich twierdzeń.
[^6]: Jest to dokładnie ta definicja, którą poznają uczniowie szkoły średniej: prawdopodobieństwo danego zdarzenia jest równe stosunkowi liczby elementarnych zdarzeń sprzyjających temu zdarzeniu do liczby wszystkich zdarzeń elementarnych.
[^7]: Zob. H. Reichenbach, *The Theory of Probability*, tłum. (na język angielski) E.H. Hutton, M. Reichenbach, Berkeley-Los Angeles 1949. Poszerzone wydanie wcześniejszego *Wahrscheinlichkeitslehre*, Leiden 1935.
[^8]: Zob. R. von Mises, *Probability, Statistics, and Truth*, Dover-New York 1957.
[^9]: R. Carnap, *Logical Foundations of Probability*, Chicago 1950
[^10]: Liczba opisów indywiduowych o tym samym ciągu  Q-liczb $N_1, \dots, N_k$ wyraża się wzorem: $\frac{N!}{N_1!\cdot N_2! \cdot \dots \cdot N_k!}$.
[^11]: Sam Carnap zamiast litery $P$ na oznaczenie prawdopodobieństwa używał $m*$ (od słowa "*measure*" - miara). Litera $C$ pochodzi od *confirmation*.